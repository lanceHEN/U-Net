{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "! pip install -r '../requirements.txt'",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.595140Z",
     "start_time": "2025-01-21T02:51:54.063540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from src.model import UNet\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms.functional as TF"
   ],
   "id": "e35709b22f291af0",
   "outputs": [],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.595487Z",
     "start_time": "2025-01-21T02:51:54.491881Z"
    }
   },
   "cell_type": "code",
   "source": "device=torch.device('mps')",
   "id": "a1cfc1fc5fad0b92",
   "outputs": [],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.595567Z",
     "start_time": "2025-01-21T02:51:54.880131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load image and mask files\n",
    "image_dir = '../data/images'\n",
    "mask_dir = '../data/masks'\n",
    "\n",
    "image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
    "mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('.png')])\n",
    "assert len(image_files) == len(mask_files), \"Number of images and masks must match!\""
   ],
   "id": "8fe68313a71e569",
   "outputs": [],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.595718Z",
     "start_time": "2025-01-21T02:51:55.264374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define image and mask transforms\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# resize targets to 68x68 (match model output)\n",
    "def mask_transform(mask):\n",
    "    mask = TF.resize(mask, size=(68, 68), interpolation=TF.InterpolationMode.NEAREST)\n",
    "    mask = torch.tensor(np.array(mask), dtype=torch.long, device=device)\n",
    "    mask[mask==255]=1\n",
    "    return mask\n",
    "    "
   ],
   "id": "329a7bb40eccfae",
   "outputs": [],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.595816Z",
     "start_time": "2025-01-21T02:51:55.706134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train test split\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(image_files, mask_files, test_size=0.2, random_state=42)"
   ],
   "id": "bd9da48efd7eb6ae",
   "outputs": [],
   "execution_count": 192
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.595887Z",
     "start_time": "2025-01-21T02:51:56.171700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create torch dataset to easily load and preprocess images\n",
    "class SegmentationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, mask_paths, image_transform=None, mask_transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load image and mask\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        mask = Image.open(self.mask_paths[idx]).convert(\"L\")  # Grayscale\n",
    "\n",
    "        # apply transformations\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask"
   ],
   "id": "bed277edd4f186e4",
   "outputs": [],
   "execution_count": 193
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.595948Z",
     "start_time": "2025-01-21T02:51:56.712405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define train and val datasets and dataloaders\n",
    "train_dataset = SegmentationDataset(train_images, train_masks, image_transform=image_transform, mask_transform=mask_transform)\n",
    "val_dataset = SegmentationDataset(val_images, val_masks, image_transform=image_transform, mask_transform=mask_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print('size of training data', len(train_dataset))\n",
    "print('size of validation data', len(val_dataset))\n",
    "\n",
    "# check data\n",
    "for images, masks in train_loader:\n",
    "    print(f\"Image batch shape: {images.shape}\")\n",
    "    print(f\"Mask batch shape: {masks.shape}\")\n",
    "    break"
   ],
   "id": "b2c1c89eef0380fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training data 2075\n",
      "size of validation data 519\n",
      "Image batch shape: torch.Size([8, 3, 256, 256])\n",
      "Mask batch shape: torch.Size([8, 68, 68])\n"
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.596021Z",
     "start_time": "2025-01-21T02:51:57.505704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# finally start training the model\n",
    "\n",
    "num_classes = 2\n",
    "print(\"train data size:\", len(train_dataset))\n",
    "print(\"val data size:\", len(val_dataset))"
   ],
   "id": "bfd338968740e6e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: 2075\n",
      "val data size: 519\n"
     ]
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.596083Z",
     "start_time": "2025-01-21T02:51:57.970963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get checkpoint to save model\n",
    "#checkpoint = torch.load('../checkpoints/checkpoint.pth')"
   ],
   "id": "82f855ba28ae2c1f",
   "outputs": [],
   "execution_count": 196
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.596144Z",
     "start_time": "2025-01-21T02:51:58.409030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# init model\n",
    "unet = UNet(num_classes)\n",
    "#unet.load_state_dict(checkpoint['state_dict'])\n",
    "unet = unet.to(device)"
   ],
   "id": "adc48db79738c088",
   "outputs": [],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.596207Z",
     "start_time": "2025-01-21T02:51:59.043762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# init loss with class weights\n",
    "def calculate_class_weights(data_loader, num_classes):\n",
    "    pixel_counts = torch.zeros(num_classes)  # To store counts for each class\n",
    "\n",
    "    # Loop through the dataset\n",
    "    for _, masks in data_loader:\n",
    "        # Count pixels per class\n",
    "        for c in range(num_classes):\n",
    "            pixel_counts[c] += masks[:,c,:,:].sum()\n",
    "        \n",
    "        print(pixel_counts)\n",
    "\n",
    "    # Total pixels in the dataset\n",
    "    total_pixels = pixel_counts.sum()\n",
    "\n",
    "    # Inverse frequency\n",
    "    class_weights = total_pixels / (num_classes * pixel_counts)\n",
    "    return class_weights\n",
    "\n",
    "#class_weights = calculate_class_weights(train_loader, num_classes)\n",
    "class_weights = torch.tensor([0.6341, 2.3639])\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ],
   "id": "27480321af5a44d6",
   "outputs": [],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.596268Z",
     "start_time": "2025-01-21T02:51:59.519333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# init optimizer\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=1e-4)\n",
    "#optimizer.load_state_dict(checkpoint['optimizer'])"
   ],
   "id": "ad5c33fc6a91ac8d",
   "outputs": [],
   "execution_count": 199
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.596345Z",
     "start_time": "2025-01-21T02:51:59.967116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# use to calculate mean intersection over union for evaluating performance (with one hot encodings)\n",
    "def get_miou(predictions, targets):\n",
    "    # confusion matrix between predicted and ground truth class labels\n",
    "    conf_matrix = torch.zeros(num_classes, num_classes, device=device)\n",
    "\n",
    "    predictions = predictions.flatten()\n",
    "    targets = targets.flatten()\n",
    "\n",
    "    #print('pred',predictions)\n",
    "    #print('targets',targets)\n",
    "\n",
    "    # rows are predictions, cols are targets\n",
    "    for c in range(num_classes):\n",
    "        for t in range(num_classes):\n",
    "            conf_matrix[c,t] += torch.sum((predictions == c) & (targets == t))\n",
    "\n",
    "    #print(conf_matrix)\n",
    "\n",
    "    # get iou for each, then calculate mean later\n",
    "    class_ious = []\n",
    "    for c in range(num_classes):\n",
    "        tp = conf_matrix[c,c]\n",
    "        fp = conf_matrix[c,:].sum() - conf_matrix[c,c]\n",
    "        fn = conf_matrix[:,c].sum() - conf_matrix[c,c]\n",
    "\n",
    "        U = tp + fp + fn\n",
    "        #print(U)\n",
    "\n",
    "        if U == 0: # prevent division by 0\n",
    "            class_ious.append(float('nan'))\n",
    "        else:\n",
    "            class_ious.append(tp.float()/U)\n",
    "\n",
    "    #print(class_ious)\n",
    "\n",
    "    return torch.nanmean(torch.tensor(class_ious, device=device))"
   ],
   "id": "f758c5efcd38b0cd",
   "outputs": [],
   "execution_count": 200
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.596403Z",
     "start_time": "2025-01-21T02:52:00.393499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# use to evaluate with val data\n",
    "def eval_model(model, val_loader, criterion):\n",
    "    loss_sum = 0\n",
    "    miou_sum = 0\n",
    "    batches = len(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            out = model(images)\n",
    "            loss = criterion(out.to('cpu'), masks.to('cpu'))\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(out, dim=1)\n",
    "            miou = get_miou(predictions, masks.to(device)).item()\n",
    "            miou_sum += miou\n",
    "\n",
    "    avg_val_loss = loss_sum / batches\n",
    "    avg_val_miou = miou_sum / batches\n",
    "    return avg_val_loss, miou_sum"
   ],
   "id": "4732c70754448d41",
   "outputs": [],
   "execution_count": 201
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.596460Z",
     "start_time": "2025-01-21T02:52:00.828493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# init summary writer\n",
    "writer = SummaryWriter('../runs/unet')"
   ],
   "id": "930a492be39f399",
   "outputs": [],
   "execution_count": 202
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-01-21T03:00:14.596515Z",
     "start_time": "2025-01-21T02:52:01.220567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# training loop\n",
    "\n",
    "cur_epoch = 0\n",
    "#cur_epoch = checkpoint['epoch']\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(cur_epoch, cur_epoch + epochs):\n",
    "    mious = 0\n",
    "    losses = 0\n",
    "    n = len(train_loader)\n",
    "    for batch_idx, (images, masks)in enumerate(train_loader):\n",
    "        #print(images.shape, masks.shape)\n",
    "        #print(torch.unique(masks))\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        out = unet(images)\n",
    "        out = out.to(device)\n",
    "        #print(out.shape)\n",
    "        #print(masks.shape)\n",
    "        #print(f\"Output device: {out.dtype}\")\n",
    "        #print(f\"Mask device: {masks.dtype}\")\n",
    "        # have to put temporarily on cpu as torch has some sort of issue with mps here :(\n",
    "        loss = criterion(out.to('cpu'), masks.to('cpu'))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # turn probabilities into hard predictions\n",
    "        predictions = torch.argmax(out, dim=1)\n",
    "\n",
    "        print('loss',loss.item())\n",
    "        #print('pred',torch.unique(predictions))\n",
    "        #print('mask',torch.unique(masks))\n",
    "        #print('out',torch.unique(out))\n",
    "        miou = get_miou(predictions, masks).item()\n",
    "        print('miou',miou)\n",
    "\n",
    "        losses += loss.item()\n",
    "        mious += miou\n",
    "\n",
    "        # add train and mIoU for individual\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * n + batch_idx)\n",
    "        writer.add_scalar('mIoU/train', miou, epoch * n + batch_idx)\n",
    "\n",
    "    # add train loss and mIoU for batch\n",
    "    print(\"epoch mIoU:\",mious/n)\n",
    "    print(\"epoch loss:\",losses/n)\n",
    "    writer.add_scalar(' train loss/epoch', losses/n, epoch)\n",
    "    writer.add_scalar('train mIoU/epoch', mious/n, epoch)\n",
    "\n",
    "    # add val loss and mIoU\n",
    "    val_loss, val_miou = eval_model(unet, val_loader, criterion)\n",
    "    writer.add_scalar('val loss/epoch', val_loss, epoch)\n",
    "    writer.add_scalar('val mIoU/epoch', val_miou, epoch)\n",
    "    if epoch % 3 == 0:\n",
    "        sample_images = images[:5]\n",
    "        sample_preds = predictions[:5]\n",
    "        sample_masks = masks[:5]\n",
    "        print(sample_images.shape, sample_masks.shape, sample_preds.shape)\n",
    "\n",
    "        writer.add_images(\"Images\", sample_images, epoch)\n",
    "        writer.add_images(\"Predictions\", sample_preds.unsqueeze(1), epoch)\n",
    "        writer.add_images(\"Masks\", sample_masks.unsqueeze(1), epoch)\n",
    "\n",
    "    # save progress\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': unet.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, '../checkpoints/checkpoint.pth')"
   ],
   "id": "10c76675fdd025a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.6949171423912048\n",
      "miou 0.40026551485061646\n",
      "loss 0.6989352107048035\n",
      "miou 0.3800819516181946\n",
      "loss 0.6814723610877991\n",
      "miou 0.47411423921585083\n",
      "loss 0.6883294582366943\n",
      "miou 0.4058724045753479\n",
      "loss 0.6892361640930176\n",
      "miou 0.4031760096549988\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# close writer\n",
    "writer.close()"
   ],
   "id": "8793425a3dbc0af5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
